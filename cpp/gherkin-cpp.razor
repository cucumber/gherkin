@using Berp;
@functions {
public static string ToSnakeCase(string str)
{
    return
        string.Concat(
            str.Select(
                (x, i) => i > 0 && char.IsUpper(x)
                ? "_" + x
                : x.ToString()
            )
        ).ToLower();
}

public static string NameOf(Rule rule)
{ return ToSnakeCase(rule.Name.Replace("#", "")); }
}
@helper CallProduction(ProductionRule production)
{
  switch(production.Type)
  {
    case ProductionRuleType.Start:
      @:  start_rule(context, Rule_@production.RuleName);
      break;
    case ProductionRuleType.End:
      @:  end_rule(context, Rule_@production.RuleName);
      break;
    case ProductionRuleType.Process:
      @:  build(context, token);
      break;
  }
}
@helper HandleParserError(IEnumerable<string> expectedTokens, State state)
{<text>
    /* "State: @state.Id - @Raw(state.Comment)" */
    std::string expected_tokens = L"@Raw(string.Join(", ", expectedTokens))";

    auto error =
        token.is_eof()
        ? error_type::unexpected_eof
        : error_type::unexpected_token
        ;

    if (stop_at_first_error()) {
        throw_error(error, token, expected_tokens);
    }

    add_error(error, token, expected_tokens);

    return @state.Id;</text>}
@helper MatchToken(TokenType tokenType)
{<text>match_@(ToSnakeCase(tokenType.Name))(context, token)</text>}
// This file is generated. Do not edit! Edit gherkin-cpp-parser.razor instead.
#include <functional>

#include <gherkin/parser.hpp>
#include <gherkin/token.hpp>
#include <gherkin/rule_type.hpp>
#include <gherkin/types.hpp>

namespace gherkin {

struct parser_context
{
    ast_builder& builder;
    token_scanner& scanner;
    token_matcher& matcher;
    token_queue queue;
    strings errors;
    bool stop_at_first_error = false;

    bool has_token() const
    { return !queue.empty(); }

    token pop_token()
    {
        auto t = std::move(queue.front());
        queue.pop_front();

        return t;
    }

    void push_tokens(const token_queue& q)
    { queue.insert(queue.end(), q.begin(), q.end()); }

    bool has_errors() const
    { return !errors.empty(); }

    void add_error(const std::string& e)
    { errors.push_back(e); }

    void add_error(const std::exception& e)
    { add_error(e.what()); }
};

using match_function = std::function<std::size_t(parser_context&, token&)>;
using match_functions = std::unordered_map<std::size_t, match_function>;

static token read_token(parser_context& context);

static void start_rule(parser_context& context, rule_type rule_type);

static void end_rule(parser_context& context, rule_type rule_type);

static int match_token(int state, token& token, parser_context& context);

template <typename Argument, typename Action>
bool
handle_external_error(
    parser_context& context,
    bool default_value,
    Argument&& argument,
    Action&& action
)
{
    if (context.stop_at_first_error) {
        return action(argument);
    }

    try {
        return action(argument);
    } catch (const std::exception& e) {
        context.add_error(e);
    }

    return default_value;
}

template <typename Argument, typename Action>
void
handle_ast_error(
    parser_context& context,
    Argument&& argument,
    Action&& action
)
{ handle_external_error(context, true, argument, action); }

parser::parser(const parser_info& pi)
: pi_{pi}
{}

std::size_t
parser::parse(const file& file)
{
    builder_.reset();
    scanner_.reset();
    matcher_.reset();

    parser_context context{
        .builder = builder_,
        .scanner = scanner_,
        .matcher = matcher_
    };

    start_rule(context, rule_type::@NameOf(Model.RuleSet.StartRule));

    std::size_t state = 0;

    while (true) {
        auto token = read_token(context);
        state = match_token(state, token, context);

        if (token.is_eof()) {
            break;
        }
    }

    end_rule(context, rule_type::@NameOf(Model.RuleSet.StartRule));

    if (context.has_errors()) {
        // TODO: thow coumpound error
    }

    return 0;
}

static
token
read_token(parser_context& context)
{
    token t;

    if (context.has_token()) {
        t = context.pop_token();
    } else {
        t = context.scanner.read();
    }

    return t;
}

static
void
build(parser_context& context, token& token)
{ context.builder.build(token); }

namespace detail {

@foreach(var rule in Model.RuleSet.TokenRules)
{
<text>
bool
match_@(NameOf(rule))(parser_context& context, token& token)
{
    @if (rule.Name != "#EOF")
    {
    @:if (token.is_eof()) {
    @:    return false;
    @:}
    @:
    }
    return
        handle_external_error(
            context,
            false,
            token,
            [&context](auto& t) {
                return context.matcher.match_@(NameOf(rule))(t);
            }
        );
}
</text>
}

@foreach(var state in Model.States.Values.Where(s => !s.IsEndState))
{<text>
// @Raw(state.Comment)
std::size_t
match_token_at_@(state.Id)(parser& parser, token& token, parser_context& context)
{
@{var indent = "";}
    @foreach(var transition in state.Transitions)
    {
    @:if (@MatchToken(transition.TokenType)) {
        if (transition.LookAheadHint != null)
        {
        @:if (lookahead_@(transition.LookAheadHint.Id)(context, token)) {
            indent = "    ";
        }
        foreach(var production in transition.Productions)
        {
            @indent@CallProduction(production)
        }
        @:@(indent)return transition.TargetState;
        if (transition.LookAheadHint != null)
        {
        @:}
        }
    @:}
    }

    @HandleParserError(state.Transitions.Select(t => "#" + t.TokenType.ToString()).Distinct(), state)
}
</text>
}

@foreach(var lookAheadHint in Model.RuleSet.LookAheadHints)
{
<text>
bool
lookahead_@(lookAheadHint.Id)(parser_context& context, token& current_token)
{
    current_token.detach();
    token token;
    token_queue queue;
    bool match = false;

    while (true) {
        token = context.read_token();
        token.detach()
        queue.push_back(token);

        if (@foreach(var tokenType in lookAheadHint.ExpectedTokens) {<text>s@MatchToken(tokenType) || </text>}false) {
            match = true;
            break;
        }

        if not (@foreach(var tokenType in lookAheadHint.Skip) {<text>@MatchToken(tokenType) || </text>}false) {
            break;
        }

        context.push_tokens(queue);

        return match;
}</text>
}

} // namespace detail

std::size_t
parser::match_token(std::size_t state, token& token, parser_context& context)
{
    using match_function = int (parser::*)(token&, parser_context&);
    using match_functions = std::unordered_map<std::size_t, match_function>;

    static const match_functions = {
        { 0, &parser::}
    };

    return state;
}
